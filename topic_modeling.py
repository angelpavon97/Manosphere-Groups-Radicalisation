from IncelsSQL import IncelsSQL

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from nltk import word_tokenize, pos_tag
from nltk.stem.porter import *
from nltk.stem import WordNetLemmatizer

import scipy.sparse
from sklearn.feature_extraction.text import CountVectorizer
from gensim import matutils, models
import pickle

def get_url_regex(): # https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/11332580
    regex = r'('
    # Scheme (HTTP, HTTPS, FTP and SFTP):
    regex += r'(?:(https?|s?ftp):\/\/)?'
    # www:
    regex += r'(?:www\.)?'
    regex += r'('
    # Host and domain (including ccSLD):
    regex += r'(?:(?:[A-Z0-9][A-Z0-9-]{0,61}[A-Z0-9]\.)+)'
    # TLD:
    regex += r'([A-Z]{2,6})'
    # IP Address:
    regex += r'|(?:\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
    regex += r')'
    # Port:
    regex += r'(?::(\d{1,5}))?'
    # Query path:
    regex += r'(?:(\/\S+)*)'
    regex += r')'

    return regex

def remove_urls(text):
    regex = get_url_regex()
    find_urls = re.compile(regex, re.IGNORECASE)

    found_urls = [f[0] for f in find_urls.findall(text)]
    
    for url in found_urls:
        text = text.replace(url, '')

    return text
    
def is_noun_or_adj(pos):
    return pos[:2] == 'NN' or pos[:2] == 'JJ'

def process_text(text):

    # Remove urls
    text = remove_urls(text)

    # remove non alphanumeric characters
    text = re.sub('[\W_]+', ' ', text, flags=re.UNICODE)

    # Tokenization
    tokenized = word_tokenize(text)

    # Stop words
    stop_words = stopwords.words('english')
    stop_words.extend(['https', 'http', 'www', 'html', 'get_simple', 'com', 'click', 'archive', 'amp', 'auto', 'sub', 'url'])

    # Removing stop words and keeping adj and nouns
    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_or_adj(pos) and word not in stop_words]

    # Stemming
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in nouns_adj]

    return ' '.join(lemmatized_tokens)

def get_df_example():

    urls = ['google.es',
            'youtube.com',
            'twitter.com',
            'wikipedia.com',
            'facebook.com']
    comments = ['Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the five Big Tech companies alongside Amazon, Facebook, Apple, and Microsoft.',
                'YouTube is an online video platform owned by Google. It is the second most-visited website in the world.[7] In 2019, more than 500 hours of video content were uploaded to YouTube servers every minute. YouTube provides several ways to watch videos, including the website, the mobile apps, and permitting other websites to embed them. Available content includes video clips, TV show clips, music videos, short and documentary films, audio recordings, movie trailers, live streams, video blogs, and short original videos. Most content is generated by individuals, but media corporations also publish videos. Besides watching and uploading, registered users can comment on videos, rate them, create playlists, and subscribe to other users.',
                'Twitter is an American microblogging and social networking service on which users post and interact with messages known as "tweets". Registered users can post, like and retweet tweets, but unregistered users can only read them. Users access Twitter through its website interface or its mobile-device application software ("app"), though the service could also be accessed via SMS before April 2020. Twitter, Inc. is based in San Francisco, California, and has more than 25 offices around the world. Tweets were originally restricted to 140 characters, but was doubled to 280 for non-CJK languages in November 2017. Audio and video tweets remain limited to 140 seconds for most accounts. ',
                'Wikipedia is a free, multilingual open-collaborative online encyclopedia created and maintained by a community of volunteer contributors using a wiki-based editing system. Wikipedia is the largest general reference work on the Internet, and one of the 15 most popular websites as ranked by Alexa; in 2021, it was ranked as the 13th most-visited. The project carries no advertisements and is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.',
                'Facebook (stylized as facebook) is an American online social media and social networking service based in Menlo Park, California, and a flagship service of the namesake company Facebook, Inc. It was founded by Mark Zuckerberg, along with fellow Harvard College students and roommates Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes. The founders of Facebook initially limited membership to Harvard students. Membership was expanded to Columbia, Stanford, and Yale before being expanded to the rest of the Ivy League, MIT, NYU, Boston University, then various other universities in the United States and Canada, and lastly high school students. Since 2006, anyone who claims to be at least 13 years old has been allowed to become a registered user of Facebook, though this may vary depending on local laws. The name comes from the face book directories often given to American university students. '
    ]

    d = {'urls': urls, 'comments': comments}
    df = pd.DataFrame(data=d)

    return df

def get_df(paths=False):

    connection = IncelsSQL()
    if paths == False:
        data_dict = connection.get_comments_from_url_table()
    else:
        data_dict = connection.get_comments_from_path_table()

    df = pd.DataFrame(data_dict.items(), columns=['urls', 'comments'])

    return df

def get_document_term_matrix(df, cv):

    data_cv = cv.fit_transform(df.comments)
    dt_matrix = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())
    dt_matrix.index = df.index

    return dt_matrix

def apply_lda(corpus, id2word, num_topics, passes, save_model = True):

    # LDA
    lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=passes)

    # Save model
    if save_model == True:
        file_name = 'lda_' + str(num_topics) + '_topics_' + str(passes) + '_passes.sav'
        pickle.dump(lda, open('./topic_modeling/models/' + file_name, 'wb'))

    # Get topics
    topics = lda.print_topics()

    # Assigned topics
    corpus_transformed = lda[corpus]

    assigned_topics = []
    for c in corpus_transformed:        
        assigned_topics.append(sorted(c, key=lambda tup: tup[1], reverse=True)) # Get topics and percentages

    assigned_topics = list(zip(assigned_topics, dt_matrix.index)) # Join urls with their topics

    return topics, assigned_topics


def get_topics(dt_matrix, cv, num_topics = 4, passes = 10):

    # Create the gensim corpus
    corpus = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(dt_matrix.transpose()))

    # Create the vocabulary dictionary
    id2word = dict((v, k) for k, v in cv.vocabulary_.items())

    # Apply LDA
    topics, assigned_topics = apply_lda(corpus, id2word, num_topics, passes)

    return topics, assigned_topics

def save_topics(file_name, topics, assigned_topics):
    f = open('./topic_modeling/topics/' + file_name, 'w')

    f.write('OBTAINED TOPICS:\n\n')
    for t in topics:
        f.write(str(t[0]) + '\t\t' + str(t[1]) + '\n')

    f.write('\nASSIGNED TOPICS:\n\n')
    for a in assigned_topics:
        f.write(str(a[1]) + '\t\t\t' + str(a[0]) + '\n')

    f.close()

def get_topics_matrix(assigned_topics, n_topics):

    n_urls = len(assigned_topics)
    m = np.zeros((n_urls, n_topics))
    labels = []

    for i,a in enumerate(assigned_topics):
        labels.append(a[1])

        for topic, percentage in a[0]:
            m[i][topic] = percentage

    return m, labels
    
def save_topics_matrix(file_name, assigned_topics, n_topics):
    
    m, labels = get_topics_matrix(assigned_topics, n_topics)

    fig = plt.figure(figsize=(4,30))
    ax = fig.add_subplot(111)
    cax = ax.matshow(m, interpolation='nearest', aspect='auto')
    fig.colorbar(cax)

    ax.set_xticks([r for r in range(n_topics)])
    ax.set_xticklabels([r for r in range(n_topics)])
    ax.set_yticks(np.arange(len(labels)))
    ax.set_yticklabels(['']+labels)

    # plt.show()

    fig.savefig('./topic_modeling/matrixes/' + file_name, bbox_inches = 'tight')

def apply_RCD(assigned_topics):
    # Apply Ranking-based Clustering
    clusters_RCD = {}

    for a in assigned_topics:
        url = a[1] 
        highest_topic = a[0][0][0] # cluster name

        if highest_topic in clusters_RCD:
            clusters_RCD[highest_topic].append(url)
        else:
            clusters_RCD[highest_topic] = [url]

    return clusters_RCD

def apply_CRDC(assigned_topics, threshold = 0.98):
    # Apply Cumulative Ranking-based Clustering
    clusters_CRDC = {}

    for a in assigned_topics:
        url = a[1]
        cluster_name = ''

        sum = 0

        for t in a[0]:
            cluster_name += str(t[0])
            sum += t[1]

            if sum > threshold:
                break

        if cluster_name in clusters_CRDC:
            clusters_CRDC[cluster_name].append(url)
        else:
            clusters_CRDC[cluster_name] = [url]

    return clusters_CRDC

def save_topics_clustering(file_name, assigned_topics, threshold = 0.98):

    # Clustering 1: Ranking-based clustering
    clusters_RCD = apply_RCD(assigned_topics)

    f = open('./topic_modeling/clustering/RCD/' + file_name, 'w')
    f.write('RANKING-BASED CLUSTERING\n')

    for c_name, url_list in clusters_RCD.items():
        f.write('\nCluster ' + str(c_name))

        for url in url_list:
            f.write('\n\t' + url)

    f.close()

    # Clustering 2: Cumulative Ranking-based Clustering
    clusters_CRDC = apply_CRDC(assigned_topics, threshold)
    
    f = open('./topic_modeling/clustering/CRCD/' + file_name, 'w')
    f.write('CUMULATIVE RANKING-BASED CLUSTERING\n')

    for c_name, url_list in clusters_CRDC.items():
        f.write('\nCluster ' + str(c_name))

        for url in url_list:
            f.write('\n\t' + url)

    f.close()

# MAIN
if __name__ == "__main__":

    # Set paths
    paths = True

    # Get data
    print('Getting data...')
    df = get_df(paths)
    print(df)

    # Process comments
    print('Processing comments...')
    processed_comments = pd.DataFrame(df.comments.apply(process_text))
    processed_comments.index = df.urls

    # Counter vectorizer model
    print('Creating counter vectorizer model...')
    cv = CountVectorizer(max_df=.75)

    # Get document term matrix
    print('Getting document term matrix...')
    dt_matrix = get_document_term_matrix(processed_comments, cv)

    # Get topics
    n_topics = [8, 9, 10, 11, 12]
    passes = [100, 500, 1000]

    for n in n_topics:
        for p in passes:
            # Get assigned topics
            print('Getting ' + str(n) + ' topics with ' + str(p) + ' passes...')
            topics, assigned_topics = get_topics(dt_matrix, cv, num_topics=n, passes=p)

            # Save topics
            print('Saving ' + str(n) + ' topics with ' + str(p) + ' passes...')
            if paths == True:
                file_name = 'incelsPaths_' + str(n) + '_topics_' + str(p) + '_passes'
            else:
                file_name = 'incelsURLS_' + str(n) + '_topics_' + str(p) + '_passes'

            save_topics(file_name + '.txt', topics, assigned_topics)
            save_topics_matrix(file_name + '.png', assigned_topics, n)
            save_topics_clustering(file_name + '.txt', assigned_topics)

            print('Topics saved successfully.')